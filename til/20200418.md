# TIL

## DSND - NLP

### COMMON WORKFLOW

1. Normalize
2. Tokenize, Tagging and entity component
3. Lemmatize
4. Stemming

### Representing Data

Then to represent a sentences there are multiple ways to do it:
- WordNet
- Word2Vec
#### Bag of Words

This will represent the sentence into how many times a unique word occurs in the sentences represented by vector. And to calculate the similarity between sentences, a cosine measure can be used. (a.b/|a|*|b|)

#### TF-IDF

Term Frequency and Inverse Document Frequency
TF = number of term appear in each document per number of documents
IDF = Log of the Number of document per the number of document with the term

#### Word2Vec

Given neighboring word (continuous bag of words)
given a word then predict the other (skip-gram)

#### GloVe

Global Vectors for Word Representation : using statistics count the probability of a word in a context o another word. It is a huge matrix.

#### t-SNE

to visualize high dimensial vector to low dimensional one. WHen performing transform it preserve the linear relationship as opposed to PCA.
